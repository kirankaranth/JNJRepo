from __future__ import print_function

from sqlalchemy.sql.expression import select, literal_column
from sqlalchemy import MetaData, Table, text, create_engine, join, inspect
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.engine import reflection
from sqlalchemy.orm import sessionmaker
from DatabaseLibrary import DatabaseLibrary
from itertools import chain
from robot.api import logger
import ast
import os
import pyodbc
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext
from pyspark import SparkContext
import re
import pandas as pd
from tabulate import tabulate
import json
import shutil
import datetime
import traceback

Base = declarative_base()
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)
pd.set_option("display.width", None)
pd.set_option("display.max_colwidth", -1)
pd.set_option("display.colheader_justify", "left")
# sc = SparkContext()
# sqlContext = SQLContext(sc)


class OrmQuery(DatabaseLibrary):
    def __init__(self):
        self.table_model = None
        self.column_names = []
        self.conn = None
        self.meta = None
        self.engine = None
        self.session = None
        self.query_list = []

    def connect_using_orm(self, server, database, uid, password):
        """
        Connects using SQLAlchemy for providing further functionality.
        For reference see: https://docs.sqlalchemy.org/en/13/core/connections.html

        :param server: Server name to be connected to database.
                        E.g. in Dev it is: mdh-dev-sqlserver.database.windows.net
        :param database: Database to connect to in Azure environment. E.g. MDH-CORE
        :param uid: Username to connect to Azure DB
        :param password: Password to connect to Azure DB
        """
        try:
            driver = self._get_driver_string()
            engine_stmt = "mssql+pyodbc://%s:%s@%s/%s?driver=%s&Mars_Connection=Yes" % (
                uid,
                password,
                server,
                database,
                driver,
            )
            orm_engine = create_engine(engine_stmt)
            self._set_metadata(orm_engine)
            Session = sessionmaker(bind=orm_engine)
            self.session = Session()
            self.conn = orm_engine.connect()
            self.engine = orm_engine
        except Exception as e:
            traceback.print_exc()
            raise

    def connect_using_databricks(self, region, token, database, http_path):
        """
        Creates engine by connecting to database
        :type http_path: http path
        :param token: access token generated by databricks admin
        :param database:  database which you want to access
        :param region: physical region of the server
        :return: sets engine
        """
        # Azure Databricks with token
        # provide token, region for url, database name, http_path (with cluster name)
        try:
            dbfs_engine = create_engine(
                "databricks+pyhive://token:"
                + token
                + "@"
                + region
                + ".azuredatabricks.net:443/"
                + database,
                connect_args={"http_path": http_path},
            )
            self._set_metadata_databricks(dbfs_engine)
            Session = sessionmaker(bind=dbfs_engine)
            self.session = Session()
            self.engine = dbfs_engine
            self.conn = dbfs_engine.connect()
        except Exception as e:
            traceback.print_exc()
            raise

    def disconnect_databricks_conn(self):
        """
        Disconnects orm connection if it is connected.
        """
        if self.conn:
            self.conn.close()
            self.conn = None
            self.meta = None
            self.engine = None
            self.session = None

    def _set_metadata_databricks(self, engine):
        """
        Initialises metadata and binds it to the engine for databricks (do not reflect tables)
        :param engine:
        :return: sets metadata
        """
        self.meta = MetaData(bind=engine)

    def disconnect_orm(self):
        """
        Disconnects orm connection if it is connected.
        """
        if self.conn:
            self.conn.close()
            self.conn = None
            self.meta = None
            self.engine = None
            self.session = None

    def _set_engine(self, server, database, uid, password):
        """
        Creates engine by connecting to database
        :param server: server you want to connect to
        :param database:  database which you want to access
        :param uid: username
        :param password: password
        :return: sets engine
        """
        driver = self._get_driver_string()
        engine_stmt = "mssql+pyodbc://%s:%s@%s/%s?driver=%s&Mars_Connection=Yes" % (
            uid,
            password,
            server,
            database,
            driver,
        )
        self.engine = create_engine(engine_stmt)

    def _set_metadata(self, engine):
        """
        Initialises metadata and binds it to the engine
        Also reflects database i.e. automatically load all tables from the bound database
        :param engine:
        :return: sets metadata
        """
        self.metaAZ = MetaData(bind=engine, reflect=True)

    def _set_table(self, table_name):
        """
        :param table_name: takes table name from robot test and sets it as the current table object
        :return: SQLAlchemy Table object
        """
        table_and_schema = table_name.split(".")
        return Table(
            table_and_schema[1],
            self.meta,
            autoload=True,
            autoload_with=self.engine,
            schema=table_and_schema[0],
        )

    @staticmethod
    def _get_driver_string(driver_string="ODBC Driver 17 for SQL Server"):
        """
        Fetches odbc driver string to connect to the database. Searches for ODBC Driver 17,
        followed by ODBC Driver another version. Followed by SQL Server Native.
        :returns: Driver string to connect to the odbc database
        """
        drivers = pyodbc.drivers()
        if len(drivers) == 0:
            raise Exception("No ODBC Drivers found at all")

        if driver_string not in drivers:
            driver_found = False
            for driver in pyodbc.drivers():
                if driver.startswith("ODBC Driver"):
                    driver_string = driver
                    driver_found = True
                    break
            if not driver_found:
                for driver in pyodbc.drivers():
                    if driver.startswith("SQL Server Native"):
                        driver_string = driver
                        break
            if not driver_found:
                raise Exception("No drivers found for ODBC")
        logger.info("Utilizing odbc string: %s" % driver_string)
        driver_string = driver_string.replace(" ", "+")
        return driver_string

    def get_columns_from_table(self, table_name):
        table_model = self._set_table(table_name)
        return list(table_model.columns)

    def (self):
        """
        Uses Inspector to reflect all tables from engine
        Scans all schemas and databases
        :return: list of table objects
        """_return_table_obj_of_all_tables_in_database
        insp = reflection.Inspector.from_engine(self.engine)
        list_of_tables = []
        for schema in insp.get_schema_names():
            for table_name in insp.get_table_names(schema=schema, order_by=None):
                table = Table(
                    table_name,
                    self.meta,
                    autoload=True,
                    autoload_with=self.engine,
                    schema=schema,
                )
                list_of_tables.append(table)
        return list_of_tables

    def return_list_of_tables_all_databases(self):
        """
        Uses Inspector to reflect all tables from engine
        Scans all schemas and databases
        :return: list of table objects
        """
        insp = reflection.Inspector.from_engine(self.engine)
        list_of_tables = []
        for schema in insp.get_schema_names():
            for table_name in insp.get_table_names(schema=schema, order_by=None):
                list_of_tables.append(table_name)
        return list_of_tables

    def return_table_list_in_database(self, system):
        """
        Returns location on dbfs for a given table
        :return: list of tables in the database
        """
        inspector = inspect(self.engine)
        list_of_tables = []
        for table_name in inspector.get_table_names(schema=system):
            list_of_tables.append(table_name)
        return list_of_tables

    def return_location_of_table_on_dbfs(self, table_name, schema):
        """
        Returns location on dbfs for a given table
        :param schema: name of the schema
        :param table_name: name of table
        :return: location of table on dbfs
        """
        location_on_dbfs = ""
        query = text(f"DESCRIBE EXTENDED {schema}.{table_name}")
        self.add_result_to_report(query)
        for row in self.session.execute(query):
            row_as_dict = dict(row)
            if row_as_dict["col_name"] == "Location":
                location_on_dbfs = row_as_dict["data_type"]
        return location_on_dbfs

    def return_primary_keys_from_table(self, table_name):
        """
        Returns primary keys for a given table
        :param table_name: Name of table
        :return: list of primary keys
        """
        table_model = self._set_table(table_name)
        for primary_key in table_model.primary_key:
            print("Primary key: " + str(primary_key))
        return table_model.primary_key.columns.values()[0].name

    def return_foreign_keys_from_table(self, table_name):
        """
        Returns foreign keys for a given table
        :param table_name: Name of table
        :return: list of foreign keys
        """
        table_model = self._set_table(table_name)
        for fkey in table_model.foreign_keys:
            print("Foreign key: " + str(fkey))

    def return_column_descriptions_from_table(self, table_name, schema):
        """
        Uses Inspector to reflect all tables from engine
        Scans all schemas and databases
        :type schema: string schema name; if omitted, uses the default schema of the database connection
        For special quoting, use quoted_name
        :type table_name: string name of the table
        :return: list of dictionaries, each representing the definition of a database column
        """
        insp = reflection.Inspector.from_engine(self.engine)
        return insp.get_columns(table_name, schema)

    def return_count_of_table_with_conditions(self, table_name, condition):
        """
        Returns a count of record in a table based on a condition
        :param table_name: table name passed from robot keyword #NB Must be in the format SCHEMA_NAME.TABLE_NAME
        :param condition: condition passed from robot keyword i.e. column_name = value
        :return: Count of query object (int)
        """
        self.table_model = self._set_table(table_name)
        q = self.session.query(self.table_model).filter(text(condition)).count()
        self.add_result_to_report(
            f"""SELECT COUNT(*) \nFROM {table_name} \nWHERE {condition}"""
        )
        self.add_result_to_report("Result :" + str(q))
        return str(self.session.query(self.table_model).filter(text(condition)).count())

    def return_count_of_table(self, table_name):
        """
        Returns a count of records in a table
        :param table_name:  table name passed from robot keyword #NB Must be in the format SCHEMA_NAME.TABLE_NAME
        :return:  Count of query objects (int)
        """
        logger.info("Running return_count_of_table")
        try:
            count = self.session.execute(
                text(f"""SELECT COUNT(*) FROM {table_name}""")
            ).fetchall()[0][0]
        except EOFError:
            system = table_name.split(".")[0]
            table = table_name.split(".")[1]
            count = self.run_spark_count_query(system, table)
        self.add_result_to_report(f"""SELECT COUNT(*) \nFROM {table_name}""")
        self.add_result_to_report("Result :" + str(count))
        return int(count)

    def return_count_of_table_version_zero(self, table_name):
        """
        Returns a count of records in a table
        :param table_name:  table name passed from robot keyword #NB Must be in the format SCHEMA_NAME.TABLE_NAME
        :return:  Count of query objects (int) from version 0
        """
        try:
            count = self.session.execute(
                text(f"""SELECT COUNT(*) FROM {table_name} VERSION AS OF 0""")
            ).fetchall()[0][0]
        except EOFError:
            system = table_name.split(".")[0]
            table = table_name.split(".")[1]
            count = self.run_spark_count_version_zero(system, table)
        self.add_result_to_report(
            f"""SELECT COUNT(*) \nFROM {table_name} VERSION AS OF 0"""
        )
        return int(count)

    def return_multiple_columns_from_table(
        self, table_name, column_names, condition, amount
    ):
        """
        :param amount: amount of columns returned
        :param condition:  condition passed from robot keyword i.e. col1 >= value1 and col2 < value2
        :type column_names: list of columns that are returned from query
        :param table_name:  table name passed from robot keyword #NB Must be in the format SCHEMA_NAME.TABLE_NAME
        :return:  Column of a query result
        """
        list_of_columns = self._return_list_of_column_names(column_names)
        q = (
            select(list_of_columns)
            .select_from(text(table_name))
            .where(text(condition))
            .limit(amount)
        )
        self.add_result_to_report(str(q))
        return self.session.execute(q)

    def return_count_multiple_columns_from_table(self, table_name, column_names):
        """
        :type column_names: list of columns that are returned from query
        :param table_name:  table name passed from robot keyword #NB Must be in the format SCHEMA_NAME.TABLE_NAME
        :return:  amount of records returned from query
        """
        logger.info("Running return_count_multiple_columns_from_table")
        try:
            count = self.session.execute(
                text(
                    f"""SELECT COUNT(*) FROM (SELECT DISTINCT {column_names} FROM {table_name})"""
                )
            ).fetchall()[0][0]
        except Exception as e:
            logger.error(
                "Exception occured in return_count_multiple_columns_from_table for table "
                + {table_name}
                + " : "
                + str(e)
            )

        self.add_result_to_report(
            f"""SELECT COUNT(*) \nFROM (SELECT DISTINCT {column_names} \n\tFROM {table_name})"""
        )
        return int(count)

    def join_2_tables_based_on_a_condition(
        self, table1, table2, column_names, clause, condition, amount
    ):
        """
        Joins 2 tables and returns an amount of columns
        :param amount: amount of records returned
        :type column_names: list of columns that are returned from query
        :param clause: clause to join the columns
        :param table1: table to select from
        :param table2: table to join
        :param condition: where condition for join
        :return: list of query results
        """
        join_table_1 = self._set_table(table1)
        join_table_2 = self._set_table(table2)
        list_of_columns = self._return_list_of_column_names(column_names)
        return self.session.execute(
            select(list_of_columns)
            .select_from(join(join_table_1, join_table_2, text(clause)))
            .where(text(condition))
            .distinct()
            .limit(amount)
        )

    def count_join_2_tables_based_on_a_condition(
        self, table1, table2, clause, condition
    ):
        """
        Joins 2 tables and returns count based on a condition
        Joins 2 tables and returns count based on a condition
        :param clause: clause to join the columns
        :param table1: table to select from
        :param table2: table to join
        :param condition: where condition for join
        :return: count of query results
        """
        join_table_1 = self._set_table(table1)
        join_table_2 = self._set_table(table2)
        return (
            self.session.query(join_table_1)
            .select_from(join(join_table_1, join_table_2, text(clause)))
            .filter(text(condition))
            .distinct()
            .count()
        )

    def check_columns_match_values_in_a_list_or_subquery(
        self, table, list_of_values, select_columns, column_name
    ):
        """
        Checks query contains a list or subquery
        :param list_of_values: list or subquery to compare
        :type select_columns: columns returned from query
        :type column_name: column to compare
        :param table: table to select from
        :return: column values that match list or subquery
        """
        select_columns = self._return_list_of_column_names(select_columns)
        return self.session.execute(
            select(select_columns)
            .select_from(text(table))
            .where(literal_column(column_name).in_(ast.literal_eval(list_of_values)))
            .distinct()
        )

    @staticmethod
    def _return_list_of_column_names(column_names):
        """
        Returns a list of columns for an amount of records in a table
        :type column_names: list of columns that are returned from query
        :return: list of column names cast to text object
        """
        if isinstance(column_names, tuple):
            list_of_columns = ",".join(map(str, chain.from_iterable(column_names)))
        else:
            list_of_columns = column_names.split(",")
        list_cast_to_text = []
        for column in list_of_columns:
            column = column.strip("")  # remove whitespaces
            list_cast_to_text.append(
                text(column)
            )  # columns in list must be cast to text object
        return list_cast_to_text

    def write_to_file(self, robot_test_tag):
        """
        Create report that prints queries used for a robot test in respective order of execution
        Regular expression checks the integrity of the robot tag i.e (4characters-4digits)
        :param robot_test_tag:  the tag of the robot test you wish to create a file for
        """
        try:
            if re.search("^[\\w]{4}-[\\d]{4}", robot_test_tag):  # pattern match check
                if len(robot_test_tag) >= 9:
                    story_num = robot_test_tag[
                        :9
                    ]  # slice first 9 characters i.e. the story number of robot tag
                else:
                    story_num = robot_test_tag
                filename = f"""Tests{os.path.sep}Reports{os.path.sep}{story_num}{os.path.sep}report-{robot_test_tag}.txt"""
                os.makedirs(
                    os.path.dirname(filename), exist_ok=True
                )  # check if directory exists and create it if not
                f = open(filename, "w", encoding="utf-8")
                f.write(
                    tabulate([[f"Robot Test: {robot_test_tag}\n"]], tablefmt="grid")
                )
                f.write("\n")
                for query_string in self.query_list:
                    f.write(f"{query_string}\n")
                f.close()
                self.query_list = []
            else:
                raise ValueError("Must provide valid Robot Tag for report")
        except Exception as e:
            traceback.print_exc()
            raise

    def add_result_to_report(self, result):
        """
        Keyword used to add a result to the list for the report
        :param result: ${result} from query passed in from robot keyword
        """
        self.query_list.append(result)

    def convert_to_df(self, result):
        self.query_list.append(
            f"Result: \n {pd.DataFrame(result, columns=result.keys()).to_string(index=False)}"
        )

    @staticmethod
    def compare_column_in_2_dfs(result1, result2, column1, column2):
        df1 = pd.DataFrame(result1, columns=result1.keys())
        df2 = pd.DataFrame(result2, columns=result2.keys())
        result = df1[[column1]].equals(df2[[column2]])
        return result

    @staticmethod
    def parse_flat_file(file_path):
        df = pd.read_csv(file_path, sep="|^/", header=0, engine="python")
        return list(str(df.columns).split("|^/"))

    def run_comparison_query(self, table, col_names, system):
        try:
            count = self.session.execute(
                text(
                    f"""Select count(*) from (Select {col_names} from {system}_STG2.{table} MINUS Select {col_names} from {system}.{table} VERSION AS OF 0)"""
                )
            ).fetchall()[0][0]
        except EOFError as eof:
            logger.console("EOFERROR occurred in run_comparison_query")
            logger.console("EOFERROR occurred in run_comparison_query: " + eof)
            count = self.run_spark_comparison_query(system, table, col_names)
        except Exception as e:
            logger.console("Exception occurred in run_comparison_query")
            logger.console("Exception occurred in run_comparison_query: " + e)
        self.add_result_to_report(
            f"""Select count(*) \nfrom (Select {col_names} \n\tfrom {system}_STG2.{table} \nMINUS Select {col_names} \nfrom {system}.{table} \nVERSION AS OF 0)"""
        )
        return int(count)

    # todo change usages to run_source_target_comparison_query
    def run_pasx_comparison_query(self, table, col_names, system):
        try:
            count = self.session.execute(
                text(
                    f"""Select count(*) from (Select {col_names} from {system}_STG2.{table} MINUS Select {col_names} from {system}.{table} )"""
                )
            ).fetchall()[0][0]
        except EOFError:
            count = self.run_spark_comparison_query(system, table, col_names)
        except Exception as e:
            logger.console("Exception occurred in run_comparison_query: " + e)
        self.add_result_to_report(
            f"""Select count(*) \nfrom (Select {col_names} \n\tfrom {system}_STG2.{table} \nMINUS Select {col_names} \nfrom {system}.{table})"""
        )
        return int(count)

    # todo change usages to run_source_target_comparison_query
    def run_osipi_comparison_query(self, table, col_names, system):
        try:
            count = self.session.execute(
                text(
                    f"""Select count(*) from (Select {col_names} from {system}_STG2.{table} MINUS Select {col_names} from {system}.{table} )"""
                )
            ).fetchall()[0][0]
        except EOFError as e:
            logger.console("EOFError occurred in run_comparison_query: " + e)
            count = self.run_spark_comparison_query(system, table, col_names)
        except Exception as e:
            logger.console("Exception occurred in run_comparison_query: " + e)
        self.add_result_to_report(
            f"""Select count(*) \nfrom (Select {col_names} \n\tfrom {system}_STG2.{table} \nMINUS Select {col_names} \nfrom {system}.{table})"""
        )
        return int(count)

    def run_source_target_comparison_query(self, table, col_names, system):
        """
        Compares the difference in counts between the source(_stg) and target folder.
        :param table: the table under test
        :param col_names: the columns under test
        :param system: the system under test
        :return: an integer representation of the difference in count between source and target
        """
        try:
            # This will have an impact if we're doing joins. The spark default is 4gb (4294967296)
            # In this case, it prevents the driver OOM conditions such as:
            # java.util.concurrent.ExecutionException:org.apache.spark.sql.execution.OutOfMemorySparkException:
            #          Size of broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxResultSize=4294967296.
            #          You can disable broadcasts for this query using set spark.sql.autoBroadcastJoinThreshold=-1
            self.session.execute(text("SET spark.sql.autoBroadcastJoinThreshold=-1"))

            count = self.session.execute(
                text(
                    f"""Select count(*) from (Select {col_names} from {system}_STG2.{table} MINUS Select {col_names} from {system}.{table} VERSION AS OF 0)"""
                )
            ).fetchall()[0][0]
        except EOFError as e:
            logger.console(
                f"EOFError occurred in run_source_target_comparison_query for : {table}: "
                + str(e)
            )
            return -1
        except Exception as e:
            logger.console(
                f"Exception occurred in run_source_target_comparison_query for {table}: "
                + str(e)
            )
            return -1

        self.add_result_to_report(
            f"""Select count(*) \nfrom (Select {col_names} \n\tfrom {system}_STG2.{table} \nMINUS Select {col_names} \nfrom {system}.{table} VERSION AS OF 0)"""
        )

        if isinstance(count, int) == False:
            logger.console(f"ERROR: Result is not an integer: {count}")
            return -1

        return int(count)

    def run_count_query_with_condition(self, table, column, system, condition):
        """
        Run a count query on a table with a given condition
        :param table: the table under test
        :param system: the system under test
        :param column: the column to count
        :param condition: the condition to check
        :return: a count of rows returned from the query
        """
        try:
            count = self.session.execute(
                text(
                    f"""Select count({column}) from {system}.{table} where {column} {condition}"""
                )
            ).fetchall()[0][0]
        except EOFError as e:
            logger.console("EOF ERROR occurred in run_count_query_with_condition: ")
            logger.console(e)
            count = self.run_spark_comparison_query(system, table, column)
        except Exception as e:
            logger.error(
                f"Exception occurred in run_count_query_with_condition for {table}: "
                + str(e)
            )
            count = self.run_spark_comparison_query(system, table, column)

        self.add_result_to_report(
            f"""Select count({column}) from {system}.{table} where {column} {condition}"""
        )
        self.add_result_to_report("Result :" + str(count))

        return int(count)

    @staticmethod
    def run_spark_comparison_query(system, table, col_names):
        logger.console("[DEBUG]----> Running spark_comparison_query............")
        logger.console("[DEBUG]----> " + table)

        spark = SparkSession.builder.getOrCreate()
        # dbs = spark.sql("SHOW DATABASES").collect()
        # print(dbs)
        # dbs = spark.sql("Select count(*)-1 from ((Select SDCID, KEYID1, KEYID2, KEYID3, PARAMLISTID, PARAMLISTVERSIONID, VARIANTID, DATASET, PARAMID, PARAMTYPE, REPLICATEID, ALIASID, MANDATORYFLAG, DATATYPES, ENTEREDVALUE, ENTEREDTEXT, ENTEREDUNITS, OPERATORRULE, TRANSFORMVALUE, TRANSFORMDT, TRANSFORMTEXT, TRANSFORMRULE, DISPLAYVALUE, DISPLAYUNITS, DISPLAYFORMAT, RANGEOPERATOR, ENTEREDQUALIFIER, ENTRYSDCID, ENTRYREFTYPEID, CALCRULE, MEASUREMENTACTIONID, RELEASEDFLAG, VALUESTATUS, CONDITION, TRANSFORMDEFERFLAG, TEXTCOLOR, USERSEQUENCE, NOTES, AUDITSEQUENCE, AUDITDEFERFLAG, TRACELOGID, CREATEDT, CREATEBY, CREATETOOL, MODDT, MODBY, S_ACOVERRIDDENFLAG, MODTOOL, S_ANALYSTID, S_QCEVALSTATUS, DISPLAYVALUEFORMAT, CALCEXCLUDEFLAG, U_ADDITIONALDPS, U_COA_TRANSLATION, U_UNKNOWNNAME, U_RETENTIONTIME, U_AUTOSEQUENCE, U_DATAENTRYDT, U_RESULT_ID, U_CDS_NAME, SDIDATAITEMID, INSTRUMENTID, INSTRUMENTFIELDID, W_INSTRUMENTID, ACTIVEFLAG, EXTERNALREFERENCE, RESULTTIMEOFFSET, U_LES_NAME, U_CDS_EXCLUDE, U_SAPBATCHINFO, ENTEREDOPERATOR, UNCERTAINTYVALUE, DEFAULTVALUE, UNCERTAINTYDISPLAYVALUE, UNCERTAINTYASYMMETRICFLAG, UNCERTAINTYVALUEUPPER, UNCERTAINTYDISPLAYVALUEUPPER, UNCERTAINTYFUNCTION, UNCERTAINTYDISPLAYFORMAT, UNCERTAINTYFUNCTIONUPPER, UNCERTAINTYDISPLAYFORMATUPPER from Elims_STG.sdidataitem)\nMINUS\n(Select SDCID, KEYID1, KEYID2, KEYID3, PARAMLISTID, PARAMLISTVERSIONID, VARIANTID, DATASET, PARAMID, PARAMTYPE, REPLICATEID, ALIASID, MANDATORYFLAG, DATATYPES, ENTEREDVALUE, ENTEREDTEXT, ENTEREDUNITS, OPERATORRULE, TRANSFORMVALUE, TRANSFORMDT, TRANSFORMTEXT, TRANSFORMRULE, DISPLAYVALUE, DISPLAYUNITS, DISPLAYFORMAT, RANGEOPERATOR, ENTEREDQUALIFIER, ENTRYSDCID, ENTRYREFTYPEID, CALCRULE, MEASUREMENTACTIONID, RELEASEDFLAG, VALUESTATUS, CONDITION, TRANSFORMDEFERFLAG, TEXTCOLOR, USERSEQUENCE, NOTES, AUDITSEQUENCE, AUDITDEFERFLAG, TRACELOGID, CREATEDT, CREATEBY, CREATETOOL, MODDT, MODBY, S_ACOVERRIDDENFLAG, MODTOOL, S_ANALYSTID, S_QCEVALSTATUS, DISPLAYVALUEFORMAT, CALCEXCLUDEFLAG, U_ADDITIONALDPS, U_COA_TRANSLATION, U_UNKNOWNNAME, U_RETENTIONTIME, U_AUTOSEQUENCE, U_DATAENTRYDT, U_RESULT_ID, U_CDS_NAME, SDIDATAITEMID, INSTRUMENTID, INSTRUMENTFIELDID, W_INSTRUMENTID, ACTIVEFLAG, EXTERNALREFERENCE, RESULTTIMEOFFSET, U_LES_NAME, U_CDS_EXCLUDE, U_SAPBATCHINFO, ENTEREDOPERATOR, UNCERTAINTYVALUE, DEFAULTVALUE, UNCERTAINTYDISPLAYVALUE, UNCERTAINTYASYMMETRICFLAG, UNCERTAINTYVALUEUPPER, UNCERTAINTYDISPLAYVALUEUPPER, UNCERTAINTYFUNCTION, UNCERTAINTYDISPLAYFORMAT, UNCERTAINTYFUNCTIONUPPER, UNCERTAINTYDISPLAYFORMATUPPER from Elims.sdidataitem VERSION AS OF 0))").collect()
        # print(dbs)
        # #count_v0 = self.session.execute(text(f"""Select * from {system}.{table} VERSION AS OF 0"""))
        df1 = spark.sql("Select " + col_names + " from " + system + "_STG2." + table)
        df2 = spark.sql(
            "Select " + col_names + " from " + system + "." + table + " VERSION AS OF 0"
        )
        logger.console("[DEBUG]--run_spark_comparison_query- df2->" + df2)
        logger.console("[DEBUG]--run_spark_comparison_query- df2->" + df1)

        count = df2.subtract(df1)
        return count.count()

    @staticmethod
    def run_spark_count_query(system, table):
        logger.console("[DEBUG]-run_spark_count_query :" + table)
        spark = SparkSession.builder.getOrCreate()
        spark.sql(f"USE {system}").collect()
        dbs = spark.sql(f"SELECT COUNT(*) FROM {system}.{table}").collect()
        print(dbs[0]["count(1)"])
        spark.stop()
        return dbs[0]["count(1)"]

    @staticmethod
    def get_kafka_updates(txt):
        return [int(s) for s in txt.split() if s.isdigit()]

    def run_distinct_query_with_condition(self, table, column, system, condition):
        """
        Run a distinct query on a table with a given condition
        :param table: the table under test
        :param system: the system under test
        :param column: the column to count
        :param condition: the condition to check
        :return: a count of rows returned from the query
        """
        try:
            distinct_result = self.session.execute(
                text(
                    f"""Select distinct({column}) from {system}.{table} where {column} {condition}"""
                )
            ).fetchall()[0][0]
        except EOFError as e:
            logger.console(
                "EOF ERROR occurred in run_distinct_query_with_condition: " + e
            )
            # count = self.run_spark_comparison_query(system, table, column)
        except Exception as e:
            logger.console(
                "Exception occurred in run_distinct_query_with_condition: " + e
            )
        self.add_result_to_report(
            f"""Select distinct({column}) from {system}.{table} where {column} {condition}"""
        )
        self.add_result_to_report("Result :" + str(distinct_result))

        return distinct_result

    def isInteger(self, value):
        # todo Method updated to return_datatype_of_column-- is still used in old tests)
        if isinstance(value, int):
            logger.console("True")
            self.add_result_to_report(f"""Value ${value} is of type int""")
            return "True"
        else:
            logger.console("False")
            self.add_result_to_report(f"""Value ${value} is not of type int""")
            return "False"

    # todo Method updated to return_datatype_of_column-- is still used in old tests)
    def check_datatype(self, value, data_type):
        type_comparision = str
        if data_type == "str":
            type_comparision = str
        elif data_type == "int":
            type_comparision = int
        else:
            logger.console("[ERROR] checking data type, entered type not recognised")

        if isinstance(value, type_comparision):
            self.add_result_to_report(f"""Value {value} is of type {data_type}""")
            return "True"
        else:
            self.add_result_to_report(f"""Value {value} is not of type {data_type}""")
            return "False"

    def return_datatype_of_column(self, system, table, column):
        try:
            distinct_result = self.session.execute(
                text(f"""describe {system}.{table} {column}""")
            ).fetchall()[1][1]
        except EOFError as e:
            logger.console("EOF ERROR occurred in return_datatype_of_column: " + e)
            # count = self.run_spark_comparison_query(system, table, column)
        except Exception as e:
            logger.console("Exception occurred in return_datatype_of_column: " + e)
        self.add_result_to_report(f"""describe {system}.{table} {column}""")
        self.add_result_to_report("Result :" + str(distinct_result))
        return distinct_result

    # todo move to util class
    def return_date_as_utc(self, system, table, column):
        """
        Runs a query to return a datetime in UTC format
        :param table: the table under test
        :param system: the system under test
        :param column: the column to return as utc
        :return: a utc format string
        """
        try:
            utc_date = self.session.execute(
                text(
                    f"""SELECT date_format((select {column} from {system}.{table} where {column} is not null limit 1),"yyyy-MM-dd'T'HH:mm:ss.SSSZ")"""
                )
            ).fetchall()[0][0]
        except EOFError as e:
            logger.console("EOF ERROR occurred in return_datatype_of_column: " + e)
            # count = self.run_spark_comparison_query(system, table, column)
        except Exception as e:
            utc_date = "NO utc date retrieved"
            logger.console("Exception occurred in return_datatype_of_column: " + e)
        self.add_result_to_report(
            f"""SELECT date_format((select {column} from {system}.{table} limit 1),"yyyy-MM-dd'T'HH:mm:ss.SSSZ"""
        )
        self.add_result_to_report("Result :" + str(utc_date))
        return utc_date

    def check_src_sys_active_flag(self, system, table):
        """
        Runs a query to check Active flag for src_sys
        :param table: the table under test
        :param system: the system under test
        :return: count of flags not set
        """
        try:
            count = self.session.execute(
                text(
                    f"""select count(*) from (select * from {system}.{table} where SRC_SYS_SHRT_NM  in ('Sustain','E2','ATLAS','Panda','eLIMS') AND Active <>1)"""
                )
            ).fetchall()[0][0]
        except EOFError as e:
            logger.console("EOF ERROR occurred in check_src_sys_active_flag ")
        except Exception as e:
            logger.console("Exception occurred in check_src_sys_active_flag")
        self.add_result_to_report(
            f"""select count(*) from (select * from {system}.{table} where SRC_SYS_SHRT_NM  in ('Sustain','E2','ATLAS','Panda','eLIMS') AND Active <>1)"""
        )
        self.add_result_to_report("Result : " + str(count))

        return True if count == 0 else False
